package ratelimit

import (
	"context"
	"testing"
	"time"

	"github.com/go-redis/redis/v8"
	"github.com/mis2git/mis2git/internal/monitoring/metrics"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

// MockRedis provides a simple in-memory Redis mock for testing
type MockRedis struct {
	data map[string][]redis.Z
}

func NewMockRedis() *MockRedis {
	return &MockRedis{
		data: make(map[string][]redis.Z),
	}
}

func (m *MockRedis) ZAdd(ctx context.Context, key string, members ...*redis.Z) *redis.IntCmd {
	if m.data[key] == nil {
		m.data[key] = make([]redis.Z, 0)
	}
	
	for _, member := range members {
		m.data[key] = append(m.data[key], *member)
	}
	
	cmd := redis.NewIntCmd(ctx)
	cmd.SetVal(int64(len(members)))
	return cmd
}

func (m *MockRedis) ZRemRangeByScore(ctx context.Context, key, min, max string) *redis.IntCmd {
	// Simple mock implementation - remove entries with score < max
	if m.data[key] == nil {
		cmd := redis.NewIntCmd(ctx)
		cmd.SetVal(0)
		return cmd
	}
	
	// For testing, we'll implement a simple removal
	removed := 0
	newData := make([]redis.Z, 0)
	
	for _, z := range m.data[key] {
		// In real Redis, this would parse the score range properly
		// For testing, we'll use a simple check
		if z.Score >= 0 { // Keep all for now
			newData = append(newData, z)
		} else {
			removed++
		}
	}
	
	m.data[key] = newData
	cmd := redis.NewIntCmd(ctx)
	cmd.SetVal(int64(removed))
	return cmd
}

func (m *MockRedis) ZCard(ctx context.Context, key string) *redis.IntCmd {
	cmd := redis.NewIntCmd(ctx)
	if m.data[key] == nil {
		cmd.SetVal(0)
	} else {
		cmd.SetVal(int64(len(m.data[key])))
	}
	return cmd
}

func (m *MockRedis) Expire(ctx context.Context, key string, expiration time.Duration) *redis.BoolCmd {
	cmd := redis.NewBoolCmd(ctx)
	cmd.SetVal(true)
	return cmd
}

func (m *MockRedis) ZRangeWithScores(ctx context.Context, key string, start, stop int64) *redis.ZSliceCmd {
	cmd := redis.NewZSliceCmd(ctx)
	if m.data[key] == nil || len(m.data[key]) == 0 {
		cmd.SetVal([]redis.Z{})
	} else {
		// Return the first element for testing
		cmd.SetVal([]redis.Z{m.data[key][0]})
	}
	return cmd
}

func (m *MockRedis) ZCount(ctx context.Context, key, min, max string) *redis.IntCmd {
	cmd := redis.NewIntCmd(ctx)
	if m.data[key] == nil {
		cmd.SetVal(0)
	} else {
		cmd.SetVal(int64(len(m.data[key])))
	}
	return cmd
}

func (m *MockRedis) Keys(ctx context.Context, pattern string) *redis.StringSliceCmd {
	cmd := redis.NewStringSliceCmd(ctx)
	keys := make([]string, 0)
	for key := range m.data {
		keys = append(keys, key)
	}
	cmd.SetVal(keys)
	return cmd
}

func (m *MockRedis) Del(ctx context.Context, keys ...string) *redis.IntCmd {
	cmd := redis.NewIntCmd(ctx)
	deleted := int64(0)
	for _, key := range keys {
		if _, exists := m.data[key]; exists {
			delete(m.data, key)
			deleted++
		}
	}
	cmd.SetVal(deleted)
	return cmd
}

func (m *MockRedis) Pipeline() redis.Pipeliner {
	return &MockPipeline{redis: m}
}

type MockPipeline struct {
	redis *MockRedis
	cmds  []redis.Cmder
}

func (p *MockPipeline) ZRemRangeByScore(ctx context.Context, key, min, max string) *redis.IntCmd {
	cmd := p.redis.ZRemRangeByScore(ctx, key, min, max)
	p.cmds = append(p.cmds, cmd)
	return cmd
}

func (p *MockPipeline) ZCard(ctx context.Context, key string) *redis.IntCmd {
	cmd := p.redis.ZCard(ctx, key)
	p.cmds = append(p.cmds, cmd)
	return cmd
}

func (p *MockPipeline) Expire(ctx context.Context, key string, expiration time.Duration) *redis.BoolCmd {
	cmd := p.redis.Expire(ctx, key, expiration)
	p.cmds = append(p.cmds, cmd)
	return cmd
}

func (p *MockPipeline) Exec(ctx context.Context) ([]redis.Cmder, error) {
	return p.cmds, nil
}

func createTestRateLimiter() (*RateLimiter, *metrics.MetricsCollector) {
	metricsCollector := metrics.NewMetricsCollector()
	
	rl := &RateLimiter{
		redis:   nil, // Will be set to mock
		metrics: metricsCollector,
		limits: map[LimitType]RateLimit{
			LimitTypeCommand: {
				Requests: 10,
				Window:   time.Minute,
			},
			LimitTypeGitHubREST: {
				Requests: 100,
				Window:   time.Hour,
			},
			LimitTypeGitHubQL: {
				Requests: 200,
				Window:   time.Hour,
			},
		},
		premiumMultipliers: map[int]float64{
			0: 1.0,
			1: 2.0,
			2: 4.0,
			3: 10.0,
		},
	}
	
	return rl, metricsCollector
}

func TestDefaultConfig(t *testing.T) {
	config := DefaultConfig()
	
	assert.Equal(t, "localhost:6379", config.RedisAddr)
	assert.Equal(t, 30, config.CommandLimit.Requests)
	assert.Equal(t, time.Minute, config.CommandLimit.Window)
	assert.Equal(t, 60, config.GitHubRESTLimit.Requests)
	assert.Equal(t, time.Hour, config.GitHubRESTLimit.Window)
	assert.Equal(t, 100, config.GitHubQLLimit.Requests)
	assert.Equal(t, 1.0, config.PremiumMultipliers[0])
	assert.Equal(t, 10.0, config.PremiumMultipliers[3])
}

func TestNewRateLimiter_InvalidRedis(t *testing.T) {
	config := Config{
		RedisAddr: "invalid:9999",
		RedisDB:   0,
	}
	
	metricsCollector := metrics.NewMetricsCollector()
	
	_, err := NewRateLimiter(config, metricsCollector)
	assert.Error(t, err)
	assert.Contains(t, err.Error(), "failed to connect to Redis")
}

func TestRateLimiter_CheckLimit_UnknownLimitType(t *testing.T) {
	rl, _ := createTestRateLimiter()
	ctx := context.Background()
	
	allowed, err := rl.CheckLimit(ctx, 12345, "unknown_limit", 0)
	assert.Error(t, err)
	assert.False(t, allowed)
	assert.Contains(t, err.Error(), "unknown limit type")
}

func TestRateLimiter_PremiumMultipliers(t *testing.T) {
	rl, _ := createTestRateLimiter()
	
	// Test that premium multipliers are applied correctly
	tests := []struct {
		premiumLevel int
		expectedMultiplier float64
	}{
		{0, 1.0},   // Free
		{1, 2.0},   // Coffee
		{2, 4.0},   // Cake
		{3, 10.0},  // Sponsor
		{99, 1.0},  // Unknown level should default to 1.0
	}
	
	for _, test := range tests {
		multiplier := rl.premiumMultipliers[test.premiumLevel]
		if multiplier == 0 {
			multiplier = 1.0 // Default behavior
		}
		assert.Equal(t, test.expectedMultiplier, multiplier)
	}
}

func TestRateLimiter_GetRemainingRequests(t *testing.T) {
	rl, _ := createTestRateLimiter()
	ctx := context.Background()
	
	// Mock Redis for testing
	mockRedis := NewMockRedis()
	
	// Test unknown limit type
	remaining, err := rl.GetRemainingRequests(ctx, 12345, "unknown", 0)
	assert.Error(t, err)
	assert.Equal(t, 0, remaining)
	
	// Test with mock data - simulate current usage
	key := "rate_limit:command_rate:12345"
	mockRedis.data[key] = []redis.Z{
		{Score: float64(time.Now().UnixNano()), Member: "request1"},
		{Score: float64(time.Now().UnixNano()), Member: "request2"},
		{Score: float64(time.Now().UnixNano()), Member: "request3"},
	}
	
	// For this test, we'll verify the calculation logic
	// Premium level 0: 10 requests, current usage would be 3, so remaining = 7
	// Premium level 1: 20 requests (2x), current usage 3, so remaining = 17
	
	// We can't easily test the full Redis integration here, but we can test the multiplier logic
	limit := rl.limits[LimitTypeCommand]
	
	// Free tier
	freeLimit := int(float64(limit.Requests) * rl.premiumMultipliers[0]) // 10 * 1.0 = 10
	assert.Equal(t, 10, freeLimit)
	
	// Coffee tier
	coffeeLimit := int(float64(limit.Requests) * rl.premiumMultipliers[1]) // 10 * 2.0 = 20
	assert.Equal(t, 20, coffeeLimit)
}

func TestRateLimiter_ResetUserLimits(t *testing.T) {
	rl, _ := createTestRateLimiter()
	ctx := context.Background()
	
	// Mock Redis
	mockRedis := NewMockRedis()
	
	// Add some test data
	mockRedis.data["rate_limit:command_rate:12345"] = []redis.Z{{Score: 1, Member: "test"}}
	mockRedis.data["rate_limit:github_rest:12345"] = []redis.Z{{Score: 1, Member: "test"}}
	mockRedis.data["rate_limit:command_rate:67890"] = []redis.Z{{Score: 1, Member: "test"}} // Different user
	
	// We can't easily test the full Redis implementation with the mock,
	// but we can verify the key pattern logic
	userID := int64(12345)
	pattern := fmt.Sprintf("rate_limit:*:%d", userID)
	assert.Equal(t, "rate_limit:*:12345", pattern)
}

func TestRateLimiter_GetGlobalSystemLoad(t *testing.T) {
	rl, metricsCollector := createTestRateLimiter()
	ctx := context.Background()
	
	// Mock Redis
	mockRedis := NewMockRedis()
	
	// Add some test data to simulate system load
	mockRedis.data["rate_limit:command_rate:user1"] = []redis.Z{
		{Score: float64(time.Now().UnixNano()), Member: "req1"},
		{Score: float64(time.Now().UnixNano()), Member: "req2"},
	}
	mockRedis.data["rate_limit:github_rest:user2"] = []redis.Z{
		{Score: float64(time.Now().UnixNano()), Member: "req1"},
	}
	
	// Test the calculation logic
	maxRequestsPerMinute := 10000
	totalRequests := 3 // 2 + 1 from mock data above
	expectedLoadFactor := float64(totalRequests) / float64(maxRequestsPerMinute) // 3/10000 = 0.0003
	
	assert.Less(t, expectedLoadFactor, 1.0)
	
	// Verify that metrics are updated (this is the important part for integration)
	metricsCollector.UpdateSystemLoadFactor(expectedLoadFactor)
	
	// We can't easily verify the exact value without deeper Prometheus integration,
	// but we can verify the method doesn't panic and accepts reasonable values
	assert.GreaterOrEqual(t, expectedLoadFactor, 0.0)
	assert.LessOrEqual(t, expectedLoadFactor, 1.0)
}

func TestLimitType_Constants(t *testing.T) {
	assert.Equal(t, LimitType("command_rate"), LimitTypeCommand)
	assert.Equal(t, LimitType("github_rest"), LimitTypeGitHubREST)
	assert.Equal(t, LimitType("github_graphql"), LimitTypeGitHubQL)
	assert.Equal(t, LimitType("global_system"), LimitTypeGlobal)
}

func TestRateLimit_Structure(t *testing.T) {
	limit := RateLimit{
		Requests: 100,
		Window:   time.Hour,
	}
	
	assert.Equal(t, 100, limit.Requests)
	assert.Equal(t, time.Hour, limit.Window)
}

// Integration test that would work with real Redis (skipped in CI)
func TestRateLimiter_Integration(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping integration test in short mode")
	}
	
	// This test would require a real Redis instance
	// It's designed to be run manually or in a full integration environment
	
	config := DefaultConfig()
	metricsCollector := metrics.NewMetricsCollector()
	
	// Try to create rate limiter with default config
	// This will fail if Redis is not available, which is expected in CI
	rl, err := NewRateLimiter(config, metricsCollector)
	if err != nil {
		t.Skipf("Skipping integration test - Redis not available: %v", err)
		return
	}
	defer rl.Close()
	
	ctx := context.Background()
	userID := int64(12345)
	
	// Reset user limits before testing
	err = rl.ResetUserLimits(ctx, userID)
	require.NoError(t, err)
	
	// Test basic rate limiting
	for i := 0; i < 5; i++ {
		err = rl.ConsumeLimit(ctx, userID, LimitTypeCommand, 0)
		assert.NoError(t, err, "Request %d should be allowed", i+1)
	}
	
	// Check current usage
	usage, err := rl.GetCurrentUsage(ctx, userID, LimitTypeCommand)
	require.NoError(t, err)
	assert.Equal(t, 5, usage)
	
	// Check remaining requests
	remaining, err := rl.GetRemainingRequests(ctx, userID, LimitTypeCommand, 0)
	require.NoError(t, err)
	assert.Equal(t, 25, remaining) // 30 - 5 = 25 (default command limit is 30)
	
	// Test premium multiplier
	remaining, err = rl.GetRemainingRequests(ctx, userID, LimitTypeCommand, 1) // Coffee tier (2x)
	require.NoError(t, err)
	assert.Equal(t, 55, remaining) // (30 * 2) - 5 = 55
}

func BenchmarkCheckLimit(b *testing.B) {
	rl, _ := createTestRateLimiter()
	ctx := context.Background()
	
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		// This will fail without real Redis, but measures the overhead
		_, _ = rl.CheckLimit(ctx, int64(i%1000), LimitTypeCommand, 0)
	}
}

func BenchmarkConsumeLimit(b *testing.B) {
	rl, _ := createTestRateLimiter()
	ctx := context.Background()
	
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		// This will fail without real Redis, but measures the overhead
		_ = rl.ConsumeLimit(ctx, int64(i%1000), LimitTypeCommand, 0)
	}
}

// Test concurrent access to rate limiter
func TestRateLimiter_ConcurrentAccess(t *testing.T) {
	rl, _ := createTestRateLimiter()
	ctx := context.Background()
	
	// Test that concurrent access doesn't cause data races
	done := make(chan bool, 10)
	
	for i := 0; i < 10; i++ {
		go func(id int) {
			defer func() { done <- true }()
			
			userID := int64(id)
			for j := 0; j < 10; j++ {
				// These will fail without Redis, but test for race conditions
				_, _ = rl.CheckLimit(ctx, userID, LimitTypeCommand, 0)
				_, _ = rl.GetCurrentUsage(ctx, userID, LimitTypeCommand)
				_, _ = rl.GetRemainingRequests(ctx, userID, LimitTypeCommand, j%4)
			}
		}(i)
	}
	
	// Wait for all goroutines
	for i := 0; i < 10; i++ {
		<-done
	}
	
	// If we get here without panics, concurrent access is safe
	assert.True(t, true)
}
